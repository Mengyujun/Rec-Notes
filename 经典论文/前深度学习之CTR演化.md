# [前深度学习时代CTR预估模型的演化之路](https://zhuanlan.zhihu.com/p/61154299)

![img](https://picb.zhimg.com/v2-28ea4ed1cf23d0711a5b40bf92f5054a_b.jpg)



基础都是LR模型， 有4种主要方向的演化方式：

1. 向下为了解决**特征交叉**的问题，演化出**PLOY2，FM，FFM**等模型；
2. **自动化特征工程**向右为了使用模型化、自动化的手段解决之前特征工程的难题，Facebook将LR与GBDT进行结合，提出了**GBDT+LR**组合模型；
3. 向左Google从online learning的角度解决模型**时效性**的问题，提出了**FTRL**；
4. 向上阿里基于样本分组的思路增加模型的非线性，提出了**LS-PLM（MLR）**模型。



## 特征交叉方向



### **POLY2——特征交叉的开始**

LR的缺陷： 表达能力有限，LR仅使用**单一特征**-一阶特征（没有特征交叉），无法利用高维信息

PLOY2模型进行特征的“暴力”组合 同样仍然是**线性模型**， 就多了一个二阶的交叉

![img](https://pic3.zhimg.com/v2-b7974df0074d069910ddb426f44d3448_b.jpg)



目标函数（省略一阶部分和sigmoid函数的部分） POLY2对**所有特征**进行了两两交叉，赋予了权重 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bh%28j_1%2C+j_2%29%7D)

其本质上仍是线性模型



缺点：

1. 对离散型数据，经常采用one-hot的方法处理id类数据，致使特征向量极度稀疏，POLY2进行无选择的特征交叉使原本就非常稀疏的特征向量更加稀疏，使得大部分**交叉特征**的权重**缺乏有效的数据进行训练**，无法收敛。
2. 参数数量由n 提升到了 n^2 极大增加了训练复杂度

即**稀疏性问题和训练复杂度问题**



### FM——隐向量特征交叉

FM二阶交叉部分：

![img](https://picb.zhimg.com/v2-37ad46126a17c2a89444028fe130601c_b.png)



可以看到，FM的二阶交叉部分相对于 PLOY2 的主要改进为：

不再是单一权重来表示交叉之后的参数， 而是使用两个向量的内积（**wj1** **·** **wj2**）取代

具体， FM为每个特征学习了一个隐权重向量（latent vector），在特征交叉时，使用两个**特征隐向量的内积**作为交叉特征的权重。



优点：

1.  复杂度上，**权重数量**由PLOY2的特征交叉 的n^2 级别（n* n个特征交叉组合） 降低到了 n*k 级别 （n个特征 每个特征有k大小的隐向量）， 训练过程中，又可以通过转换目标函数形式的方法，使FM的**训练时间复杂度**进一步降低到nk级别（即FM公式的推导得到）
2. **解决了数据稀疏性问题**，  是categorical特征，所以经过**one-hot编码**以后，不可避免的样本的数据就变得很**稀疏**，并且原先必须交叉特征必须要数据中有才能训练到（这样的训练样本少甚至没有）， 现在分别计算不同特征的隐向量表达，即使没有这样的训练数据也可以，**泛化能力大大提高**
3. 工程上，梯度下降进行学习的特点使其不失实时性和灵活性， 且容易实现serving 



### **FFM——引入特征域概念**

相对于FM， FFM模型主要引入了Field-aware， 使模型的表达能力更强



FFM二阶部分：

![img](https://pic4.zhimg.com/v2-dd1bd3fb4438060b8dab806731b53465_b.jpg)

其与FM目标函数的区别就在于隐向量由原来的 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj_1%7D) 变成了 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj_1%2Cf_2%7D) ，这就意味着**每个特征**对应的不是一个隐向量，而是对应着不同域的**一组隐向量**，当![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj_1%7D)特征与![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj_2%7D)特征进行交叉时，![[公式]](https://www.zhihu.com/equation?tex=x_%7Bj_1%7D)特征会从一组隐向量中挑出与特征![[公式]](https://www.zhihu.com/equation?tex=x_%7Bj_2%7D)的域f2对应的隐向量![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj_1%2Cf_2%7D)进行交叉。同理特征![[公式]](https://www.zhihu.com/equation?tex=x_%7Bj_2%7D)也会用与![[公式]](https://www.zhihu.com/equation?tex=x_%7Bj_1%7D)的域f1对应的隐向量进行交叉。（有疑问， 为什么是对面的f ）



FFM模型学习每个特征在**f个域上**的k维隐向量，交叉特征的权重由特征在**对方特征域**上的隐向量内积得到，权重数量共**n*k*f**个。

训练方面，由于FFM的二次项并不能够像FM那样简化，因此其复杂度为**kn^2**。



## **特征工程模型化**

### Facebook- GBDT+LR

fm系列只能做到二阶特征交叉，想要获得更高阶的特征交叉（容易发生组合爆炸和计算复杂度过高的问题）

利用**GBDT自动进行特征筛选和组合**

用GBDT构建特征工程，和利用LR预估CTR两步是**独立训练**的



GBDT自动化进行特征工程

> GBDT是由多棵**回归树**组成的树林，后一棵树利用前面树林的结果与真实结果的**残差**做为拟合目标。每棵树生成的过程是一棵标准的回归树生成过程，因此每个节点的分裂是一个自然的特征选择的过程，而多层节点的结构自然进行了有效的特征组合，也就非常高效的解决了过去非常棘手的特征选择和特征组合的问题。

之后利用GBDT模型 将输入从原始特征向量到新的离散型特征向量

![img](https://pic4.zhimg.com/v2-50d3c5bd27ee9e10f1e1606f3adc6401_b.jpg)

**特征交叉的维度**：

​		由于决策树的结构特点，事实上，**决策树的深度**就决定了**特征交叉的维度**。如果决策树的深度为4（上图），通过三次节点分裂，最终的叶节点实际上是进行了3阶特征组合后的结果



## 实时性-FTRL（google）

Google-一种在线实时训练模型的方法， FTRL本质上是**模型的训练方法**



梯度下降可以分为： batch， mini-batch, sgd

online learning来说，为了进行实时得将最新产生的样本反馈到模型中，**SGD**无疑是最合适的训练方式。

**SGD有缺陷**： 难以产生稀疏解，每次仅训练一个样本，虽然总体朝向全局最优解，微观上的运动的过程呈现布朗运动的形式，这就导致SGD会使几乎所有特征的权重非零。



对于CTR预估，**稀疏解的重要性**

1. one-hot存在，维度极高的样本特征向量（百万，千万量级），需要参数尽可能是0，轻量级模型
2. 轻量级模型 样本**部署的成本**大大降低 模型inference的计算延迟



FTRL的发展过程：

![img](https://picb.zhimg.com/v2-4f84e75d22eddc10cadb999729f1aa8f_b.jpg)



## **LS-PLM——阿里曾经的主流CTR模型**（MLR）

本质很容易理解： 分片之后再进行LR，聚类之后对不同的分类使用LR

MLR可以看做是对LR的自然推广，在LR的基础上加入聚类的思想， 它**在LR的基础上采用分而治之的思路**，先对样本进行分片，再在样本分片中应用LR进行CTR预估。

让CTR模型对不同用户群体，不用用户场景更有针对性，其实理想的方法是**先对全量样本进行聚类**，再对每个**分类施以LR模型**进行CTR预估。



**MLR公式为**

![img](https://picb.zhimg.com/v2-c7b29021b4a888dc44bef57bc5f43516_b.png)

首先用聚类函数π对样本进行分类（这里的π采用了**softmax函数，对样本进行多分类**），再用LR模型计算样本在分片中具体的CTR，然后将二者进行相乘后加和。



超参数分片数m可以较好地平衡模型的拟合与推广能力。m=1即为普通LR，m越大模型的拟合能力越强，。在实践中，阿里给出了m的经验值为12。



MLR的优势：

**表达能力强**、**稀疏性高**等两个优势：

1. **端到端的非线性学习**：从模型端自动挖掘数据中蕴藏的非线性模式，省去了大量的人工特征设计，这使得MLR算法可以端到端地完成训练，在不同场景中的迁移和应用非常轻松。
2. **稀疏性**：MLR在建模时引入了L1和L2,1范数，可以使得最终训练出来的模型具有较高的稀疏度，模型的学习和在线预测性能更好。

