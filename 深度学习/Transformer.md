百度问到的 



介绍一下 Transformer 

self-attention如何作用 为什么要除以 根号下d

为什么要做 multihead-attention?

transformer 中序列长度如何影响  