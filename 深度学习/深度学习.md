### 1. 深度学习优化器的区别，Adam和SGD 

#### 一个统一的算法优化框架

深度学习优化算法经历了 SGD -> SGDM -> NAG ->AdaGrad -> AdaDelta -> Adam -> Nadam 这样的发展历程。

> 首先定义：待优化参数： ![[公式]](https://www.zhihu.com/equation?tex=w) ，目标函数： ![[公式]](https://www.zhihu.com/equation?tex=f%28w%29) ，初始学习率 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)。
>
> 而后，开始进行迭代优化。在每个epoch ![[公式]](https://www.zhihu.com/equation?tex=t) ：
>
> 1. 计算目标函数关于当前参数的梯度： ![[公式]](https://www.zhihu.com/equation?tex=g_t%3D%5Cnabla+f%28w_t%29)
> 2. 根据历史梯度计算一阶动量和二阶动量：![[公式]](https://www.zhihu.com/equation?tex=m_t+%3D+%5Cphi%28g_1%2C+g_2%2C+%5Ccdots%2C+g_t%29%3B+V_t+%3D+%5Cpsi%28g_1%2C+g_2%2C+%5Ccdots%2C+g_t%29)，
> 3. 计算当前时刻的下降梯度： ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta_t+%3D+%5Calpha+%5Ccdot+m_t+%2F+%5Csqrt%7BV_t%7D)
> 4. 根据下降梯度进行更新： ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bt%2B1%7D+%3D+w_t+-+%5Ceta_t)

步骤3、4对于各个算法都是一致的，主要的差别就体现在1和2上。 即对梯度以及动量的使用上，是否有一阶二阶动量的使用有区别



>  关于一阶动量和二阶动量
>
> 一阶动量是各个时刻梯度方向的指数移动平均值
>
> 二阶动量——该维度上，迄今为止所有梯度值的平方和
>
>  ![[公式]](https://www.zhihu.com/equation?tex=V_t+%3D+%5Csum_%7B%5Ctau%3D1%7D%5E%7Bt%7D+g_%5Ctau%5E2)
>
> 二阶动量用来衡量历史更新频率 ，上面的第3步中： 
>
> ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta_t+%3D+%5Calpha+%5Ccdot+m_t+%2F+%5Csqrt%7BV_t%7D)
>
> 可以看出，此时实质上的学习率由 ![[公式]](https://www.zhihu.com/equation?tex=+%5Calpha) 变成了 ![[公式]](https://www.zhihu.com/equation?tex=+%5Calpha+%2F+%5Csqrt%7BV_t%7D) 。 一般为了避免分母为0，会在分母上加一个小的平滑项。因此![[公式]](https://www.zhihu.com/equation?tex=%5Csqrt%7BV_t%7D) 是恒大于0的，而且参数更新越频繁，二阶动量越大，学习率就越小。





#### Gradient Descent - 梯度下降

梯度下降是指，在给定待优化的模型参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Ed) 和目标函数 ![[公式]](https://www.zhihu.com/equation?tex=J%28%5Ctheta%29) 后，算法通过沿梯度 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla_%5Ctheta+J%28%5Ctheta%29) 的相反方向更新 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 来最小化 ![[公式]](https://www.zhihu.com/equation?tex=J%28%5Ctheta%29) ， 学习率 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta) 决定了每一时刻的更新步长

计算目标函数关于参数的梯度

![[公式]](https://www.zhihu.com/equation?tex=g_t+%3D+%5Cnabla_%5Ctheta+J%28%5Ctheta%29)



#### SGD (Stochastic Gradient Descent) 

随机梯度下降即 常用的，  最为简单，没有动量的概念 

对参数的更新即为： 

![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_%7Bi%2B1%7D%3D+%5Ctheta_t+-+%5Ceta+g_t)

SGD 的缺点

> 1. 收敛速度慢，可能在鞍点处震荡。 而且可能会在沟壑的两边持续震荡，停留在一个局部最优点
> 2. 如何合理的选择学习率是 SGD 的一大难点。



#### SGD with Momentum （SGD-M）

SGD 在遇到沟壑时容易陷入震荡。为此，可以为其引入一阶动量 Momentum[3]，加速 SGD 在正确方向的下降并抑制震荡。

> 一阶动量是各个时刻梯度方向的指数移动平均值，约等于最近 ![[公式]](https://www.zhihu.com/equation?tex=1%2F%281-%5Cbeta_1%29) 个时刻的梯度向量和的平均值。

参数更新公式：

![[公式]](https://www.zhihu.com/equation?tex=m_t+%3D+%5Cgamma+m_%7Bt-1%7D+%2B+%5Ceta+g_t)

![[公式]](https://www.zhihu.com/equation?tex=m_t+%3D+%5Cbeta_1+%5Ccdot+m_%7Bt-1%7D+%2B+%281-%5Cbeta_1%29%5Ccdot+g_t)

SGD-M 在原步长之上，增加了与上一时刻***步长***相关的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma+m_%7Bt-1%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 通常取 0.9 左右，使得  参数更新方向不仅由当前的梯度决定，也与此前累积的下降方向有关

> 增加了上一时刻的步长的作用：
>
> 1. 在梯度方向变化不大的维度， 加速更新梯度
> 2. 在梯度变化大的维度，减少此次更新的幅度（有上一步的变量做影响）
>
> 即 加速以及抑制震荡



SGD以及SGD-M 的区别：

SGD：

![img](https://pic3.zhimg.com/v2-2476080e4cdfd489ae64ae3ceeafe48b_b.jpg)

SGD-M：

![img](https://pic2.zhimg.com/v2-b9388fd6e465d82687680f9d16edcd2b_b.jpg)



#### SGD with Nesterov Acceleration （NAG）

NAG 是在SGD、SGD-M的基础上的进一步改进  改进点在于步骤1 即梯度的计算

主要思想是：

>  SGD 还有一个问题是困在局部最优的沟壑里面震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。
>
>  
>
> 因此直接不计算当前的梯度，而是使用累计动量走***下一步之后的梯度***

NAG在步骤1，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的梯度下降方向：

![[公式]](https://www.zhihu.com/equation?tex=g_t%3D%5Cnabla+f%28w_t-%5Calpha+%5Ccdot+m_%7Bt-1%7D+%2F+%5Csqrt%7BV_%7Bt-1%7D%7D%29)

然后用下一个点的梯度方向，与历史累积动量相结合，计算步骤2中当前时刻的累积动量。



#### Adagrad 

开始使用二阶动量，自适应学习率 

SGD、SGD-M均是以相同的学习率去更新 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的各个分量 ，而深度学习中大量参数的更新频率是有所区别的：

对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。

>  更新不频繁的参数， 单次步长大一些，能多学习到
>
>  更新频繁的参数，希望单次步长小一些，从而使得参数学习较为稳定，不至于被单样本影响过多

引入了二阶动量：

![[公式]](https://www.zhihu.com/equation?tex=V_t+%3D+%5Csum_%7B%5Ctau%3D1%7D%5E%7Bt%7D+g_%5Ctau%5E2)

学习率等效为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta+%2F+%5Csqrt%7Bv_t+%2B+%5Cepsilon%7D) 。对于此前频繁更新过的参数，其二阶动量的对应分量较大，学习率就较小。

缺点：

> 因为![[公式]](https://www.zhihu.com/equation?tex=%5Csqrt%7BV_t%7D) 是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。



#### AdaDelta / RMSProp

针对adagrad的缺点，即二阶动量的累积使得学习率不断缩小，则不选择累计全部历史二阶动量

即使用最近一段时间窗口内的动量累计

> 改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。
>
> 使用在SGD-M中的指数移动平均值大约就是过去一段时间的平均值：
>
> ![[公式]](https://www.zhihu.com/equation?tex=V_t+%3D+%5Cbeta_2+%2A+V_%7Bt-1%7D+%2B+%281-%5Cbeta_2%29+g_t%5E2)
>
> 避免了二阶动量持续累积、导致训练过程提前结束



#### Adam

Adam 是前述方法的集大成 

SGD-M在**SGD基础**上增加了一阶动量， AdaGrad和AdaDelta在**SGD基础**上增加了二阶动量

则Adam是  在SGD基础上 把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum。

> SGD的一阶动量：
>
> ![[公式]](https://www.zhihu.com/equation?tex=m_t+%3D+%5Cbeta_1+%5Ccdot+m_%7Bt-1%7D+%2B+%281-%5Cbeta_1%29%5Ccdot+g_t)
>
> 加上AdaDelta的二阶动量：
>
> ![[公式]](https://www.zhihu.com/equation?tex=V_t+%3D+%5Cbeta_2+%2A+V_%7Bt-1%7D+%2B+%281-%5Cbeta_2%29+g_t%5E2)

两个超参数 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cbeta_1%2C+%5Cbeta_2)，前者控制一阶动量，后者控制二阶动量。



#### Nadam

同理，按照NAG的思路，对步骤1 增加不是直接计算当前梯度，而是 

![[公式]](https://www.zhihu.com/equation?tex=g_t%3D%5Cnabla+f%28w_t-%5Calpha+%5Ccdot+m_%7Bt-1%7D+%2F+%5Csqrt%7BV_t%7D%29)

之后再结合Adam即可 

即 ： Nesterov + Adam = Nadam





### 2. 评价指标 相关知识 

#### AUC两种定义

1. roc曲线下的面积，roc曲线的纵坐标是 真阳率 横坐标是伪阳率
   涉及到混淆矩阵，一些相关定义

   ![](https://pic3.zhimg.com/80/v2-a253b01cf7f141b9ad11eefdf3cf58d3_720w.jpg?source=1940ef5c)

   其中真阳率和伪阳率的定义非被为：

   tp-rate = tp / tp+fn 

   fp-rate = fp / fp+tn 

   其实TPRate就是TP除以TP所在的列，FPRate就是FP除以FP所在的列，二者意义如下：

   - **TPRate的意义是所有真实类别为1的样本中，预测类别为1的比例。**
   - **FPRate的意义是所有真实类别为0的样本中，预测类别为1的比例。**

   我们希望分类器达到的效果是：对于真实类别为1的样本，分类器预测为1的概率（即TPRate），要大于真实类别为0而预测类别为1的概率（即FPRate），即y＞x

   如何理解机器学习和统计中的AUC？ - 无涯的回答 - 知乎 https://www.zhihu.com/question/39840928/answer/241440370 

   

2. **分别随机从正负样本集中抽取一个正样本，一个负样本，正样本的预测值大于负样本的概率**

   另外一种说法： 

   AUC的概率意义是随机取一对正负样本，正样本得分大于负样本的概率

   ROC设计的初衷非常简单：随机选择一个正样本x，随机选择一个负样本y，丢给你的判别器来打分，AUC值就表示x比y更像正样本的概率。

   此时**auc的计算**上，可以简洁地表达为：

![[img]](https://www.zhihu.com/equation?tex=auc%3D%5Csum_%7Bx%5Cin+%E6%AD%A3%E6%A0%B7%E6%9C%AC%7D%7B%5Cfrac%7B1%7D%7B%E6%AD%A3%E6%A0%B7%E6%9C%AC%E6%80%BB%E6%95%B0%7D%5Ctimes%5Cfrac%7B%E7%BD%AE%E4%BF%A1%E5%BA%A6%E5%B0%8F%E4%BA%8Ex%E7%9A%84%E8%B4%9F%E6%A0%B7%E6%9C%AC%E4%B8%AA%E6%95%B0%7D%7B%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%80%BB%E6%95%B0%7D%7D)

​		根据古典概率模型

![img](https://pic1.zhimg.com/v2-33abc4c7d3ba5146701f60bea40ebc58_b.jpg)

​		即 就是统计概率 对每一个正样本x 看有多少小于x的负样本，即计算每一个概率即可 



​	由概率解释得到的AUC的一些性质：

- 样本得分都加上某个常数，AUC不变
- 对正负样本比例不敏感，意味着你可以用负采样后的样本进行模型评估也不会差太多



3. auc的局限性以及改进

   局限性： 衡量的是整体样本间的排序能力，即不同用户对不同item的排序能力，而线上很多更多关注同一个用户的不同item之间的排序能力。

   改进： 阿里在 [Deep Interest Network](https://zhuanlan.zhihu.com/p/37576578/edit)中提到一种改进版本的 AUC 指标，用户加权平均AUC（gAUC）更能反映线上真实环境的排序能力。



#### AUC的一些理解 

参考- 乱弹机器学习评估指标AUC - 吴海波的文章 - 知乎 https://zhuanlan.zhihu.com/p/52930683

##### 排序特性

AUC指标本身和模型预测score绝对值无关， 只关注排序效果

若采用precision、F1等指标，而模型预测的score是个概率值，就必须选择一个阈值来决定哪些样本预测是1哪些是0，不同的阈值选择，precision的值会不同。



##### 点击率模型的auc低于购买转化率模型的auc

业务理解，auc衡量的是正负样本之间的预测gap， 

通常，点击行为的成本要低于购买行为，从业务上理解，**点击率模型中正负样本的差别要小于购买力模型**，即购买转化模型的正样本通常更容易被预测准。



##### 线下AUC提升为什么不能带来线上效果提升？

可能出现的错误：

1.  **样本穿越**。比如样本中有时间序类的特征，但train、test的数据切分没有考虑时间因子，则容易造成穿越。
2. 前后特征对比没有出现重大失误情况 
3. **线上的数据分布和线下的样本分布不对等**， 线上分布变化？
4. auc指标失真？ 可以考虑加入gauc指标来判断 
5. 可能没有加入position bias 因素？ 



线下AUC提升为什么不能带来线上效果提升? - 萧瑟的文章 - 知乎 https://zhuanlan.zhihu.com/p/58152702

> 1. 样本角度
>
>    1. 线上出现了较多的新样本 ，线下是基于历史数据更新的 **数据分布的不一致**（冰山下的数据很大）
>    2. 前后代码不一致 **线上线下特征**不一致 代码前后不一致 要**同一套代码和数据源抽取特征**
>    3. 历史数据由老模型产生，本身也是由bias的
>    4. 线上和线下特征不一致。例如包含时间相关特征，存在**特征穿越** **样本穿越**。或者线上部分特征缺失等等
>
> 2. 评估目标
>
>    1. AUC计算的时候，不仅会涉及同一个用户的不同item，也会涉及不同用户的不同item，而**线上排序系统**每次排序只针对**同一个用户的不同item进行打分**。（auc的局限性和 gauc）
>    2. **线下没有考虑position等偏置元素** 线上效果只跟相关性有关，是和position等偏置因素无关的。而线下一般是不同position的样本混合训练，因此线上和线下评估不对等
>
> 3. 分布变化
>
>    线上有些出价策略依赖了打分分布，例如有一些相关阈值，那么就可能产生影响。这个可以绘制**CTR概率分布图**来检查。
>
> 4. 模型有较大变化的时候，例如lr->树模型，lr->深度模型，不同网络结构的深度模型变化，这种情况容易出现，原因就是新旧模型的变化较大，预估分数变化也较大。
>
> 
>
> **解决办法：**
>
> 1. **无偏样本**作为测试集。随机样本最好，不行的话，最好不要是基于老模型产生的线上样本 尽可能利用这些对新模型有利的样本
>
> 2. 使用gauc等指标
>
> 3. 线上线下模型融合  新模型预估分数 ![[公式]](https://www.zhihu.com/equation?tex=pctr_%7Bnew%7D) *和老模型预估分数* ![[公式]](https://www.zhihu.com/equation?tex=pctr_%7Bold%7D) 直接在线上做线性融合，刚上线的时候a选取比较小，随着慢慢迭代，a慢慢放大。
>
>    ![[公式]](https://www.zhihu.com/equation?tex=pctr%3Da%2Apctr_%7Bnew%7D+%2B+%281-a%29%2Apctr_%7Bold%7D)



#### f1-score 

F1 score是精确率和召回率的调和平均

![img](https://pic3.zhimg.com/80/v2-9fc7869cc3d89aa80d6fe45cd8d9b8aa_720w.jpg)

<img src="https://img-blog.csdnimg.cn/20190420235609890.png" alt="在这里插入图片描述" style="zoom:50%;" />

注意： 是* 1/2 原来记错了 



#### 实际推荐系统线上指标



### 3. 激活函数的区别

**梯度饱和区，此时梯度会变得很小甚至接近于0**

参见很好的链接：

https://www.jianshu.com/p/22d9720dbf1a

把线性神经网络，转成非线性的，主要优点：

**提高模型鲁棒性，非线性表达能力，缓解梯度消失问题，将特征图映射到新的特征空间从何更有利于训练，加速模型收敛等问题都有很好的帮助**



激活函数的发展经历了**Sigmoid -> Tanh -> ReLU -> Leaky ReLU -> Maxout**这样的过程，还有一个特殊的激活函数**Softmax**，因为它只会被用在网络中的最后一层，用来进行最后的分类和归一化。

![img](https://pic2.zhimg.com/v2-17708ef17113fc120b045db3de3dbaac_b.jpg)







不同激活函数的对比： 

![img](https://pic4.zhimg.com/v2-665f3304d409b17471dd0b7258818e0a_b.jpg)

------

#### Sigmoid

**存在问题：**

- ***Sigmoid函数饱和使梯度消失***。当神经元的激活在接近0或1处时会饱和，在这些区域梯度几乎为0，这就会导致梯度消失，几乎就有没有信号通过神经传回上一层。

- ***Sigmoid函数的输出不是零中心的***。因为如果输入神经元的数据总是正数，那么关于![[公式]](https://www.zhihu.com/equation?tex=w)的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这将会导致梯度下降权重更新时出现z字型的下降。



#### Tanh函数

**存在问题：**

Tanh解决了Sigmoid的输出是不是零中心的问题，但仍然存在饱和问题。

**为了防止饱和，现在主流的做法会在激活函数前多做一步\*batch normalization\*，尽可能保证每一层网络的输入具有均值较小的、零中心的分布。**



#### ReLu 线性整流函数

非饱和性激活函数

比sigmoid类函数主要变化是：

1）单侧抑制；

2）相对宽阔的兴奋边界；

3）稀疏激活性。





#### **Softmax**

**数学公式：**

Softmax用于多分类神经网络输出，目的是让大的更大。函数公式是

![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%28z%29_%7Bj%7D%3D%5Cfrac%7Be%5E%7Bz_%7Bj%7D%7D%7D%7B%5Csum_%7Bk%3D1%7D%5E%7BK%7D%7Be%5E%7Bz_%7Bk%7D%7D%7D%7D%5C%5C)

示意图如下。

![img](https://pic1.zhimg.com/v2-68a7dfdf613d8cd43f0569184b206c5c_b.jpg)Softmax示意图

Softmax是Sigmoid的扩展，当类别数k＝2时，Softmax回归退化为Logistic回归。

就是如果某一个 zj 大过其他 z, 那这个映射的分量就逼近于 1,其他就逼近于 0，主要应用就是多分类。

为什么要取指数，第一个原因是要模拟 max 的行为，所以要让大的更大。第二个原因是需要一个可导的函数。



### 4. 深度学习中防止过拟合

过拟合就是模型复杂度过高，低偏差高方差

#### 缓解过拟合的方法

> 1. 数据角度
>    1. 获取更多有效的训练数据
>    2. 数据增强 （标准化 随机旋转 随机裁剪 随机噪声等）
>    3. 确保数据的真实有效性 没有出现数据穿越 
> 2. 模型角度 减小模型规模， 规模太大导致模型完美拟合数据，甚至包括噪声 
> 3. 降低特征数量
> 4. 正则化（**正则化是指修改学习算法，使其降低泛化误差而非训练误差**）
>    1. 参数正则化方法（对模型参数加正则化约束）L1， L2范数 其中L1更容易得到**稀疏解** L2范数容易求导优化
>       1. L1 使得更多参数趋于0 从而相当于减小了网络中的权重参数 ，降低了网络复杂度
>       2. L1相当于一个天然的特征选择器 
>    2. 工程技巧-Dropout以及early stopping
> 5. BN
> 6. early stopping
> 7. 集成学习方法（bagging 和 boosting）



#### Dropout

含义操作： **在训练过程中每次按一定的概率（比如50%）随机地“删除”一部分隐藏单元（神经元）**

所谓的“删除” 不是真正的删除神经元，而是将这部分神经元的 激活函数设为 0从而这些神经元不参与计算

注意每一轮次的训练 都会有不同的dropout（不是一次性的）循环中每次都是不同的drop不同的隐藏层，即相当于有多个不同的子网络

![img](https://picb.zhimg.com/v2-79650a2b0124214c0bff9fb01e7460ea_b.jpg)

为什么Dropout会防止过拟合？

1. **取平均**的作用 

   不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。

2. **减少神经元之间复杂的共适应关系** 减少对于某一个子网络 或 特征的过度依赖 

   使得有可能原有神经网络中的两个神经元不会同时出现，避免了对某个特征（某个子网络）的过度依赖，从而降低了过拟合的可能性

   不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。

3. 类似于 环境突变 但仍然能做出及时正确的反应 此时就是鲁棒的 



####  batch normalizatin

参见很好的**链接**： 

Batch Normalization原理与实战 - 天雨粟的文章 - 知乎 https://zhuanlan.zhihu.com/p/34879333

深度学习中 Batch Normalization为什么效果好？ - 言有三的回答 - 知乎 https://www.zhihu.com/question/38102762/answer/607815171



bn的**背景**是： 

1. 随着训练的进行，一方面有蝴蝶效应，底层网络的参数的小变动可能导致后面的层 的输入有很大的变化，即输入分布有较大的变化
2.  随着网络层数的加深，后面的网络层次 的输入很容易陷入梯度饱和区（此时梯度会变得很小甚至接近于0）（参考sigmoid函数的首尾部分），从而参数更新变慢，网络收敛变慢



BN的**操作**大概是：

在mini-batch的情况下，对每一层的输入【】一个长度为batch-size的list 做正则化

原先输入： 此时的batchsize 为8 

![img](https://pic4.zhimg.com/v2-084e9875d10896369e09af5a60e56250_b.jpg)

做batch normalization 后输入的变化，即输入有了分布

![img](https://pic4.zhimg.com/v2-c37bda8f138402cc7c3dd62c509d36f6_b.jpg)

**用更加简化的方式来对数据进行规范化，使得第 ![[公式]](https://www.zhihu.com/equation?tex=l) 层的输入每个特征的分布均值为0，方差为1。**让每一层网络的输入数据分布都变得稳定（解决之前传递之后输入分布不稳定的情况）

上面的缺陷： 改变了分布 可能导致数据表达能力变差

引入两个参数来改变

BN又引入了两个可学习（learnable）的参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 与 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 。这两个参数的引入是为了恢复数据本身的表达能力，对**规范化**后的数据进行**线性变换**，即 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BZ_j%7D%3D%5Cgamma_j+%5Chat%7BZ%7D_j%2B%5Cbeta_j)

若 当 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%5E2%3D%5Csigma%5E2%2C%5Cbeta%3D%5Cmu) 时，即可恢复原数据分布，从而**一定程度**上保证了输入数据的表达能力



由上可得， bn的**公式**为：

对于神经网络中的第 ![[公式]](https://www.zhihu.com/equation?tex=l) 层，我们有：

![[公式]](https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Cmu%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5EmZ%5E%7B%5Bl%5D%28i%29%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%5E2%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%28Z%5E%7B%5Bl%5D%28i%29%7D-%5Cmu%29%5E2)

![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BZ%7D%5E%7B%5Bl%5D%7D%3D%5Cgamma%5Ccdot%5Cfrac%7BZ%5E%7B%5Bl%5D%7D-%5Cmu%7D%7B%5Csqrt%7B%5Csigma%5E2%2B%5Cepsilon%7D%7D%2B%5Cbeta)

![[公式]](https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D%3Dg%5E%7B%5Bl%5D%7D%28%5Ctilde%7BZ%7D%5E%7B%5Bl%5D%7D%29)

即对每一层的输入 做标准化之后 再加了两个参数 来保证一定的输入数据的表达能力 

总结： 通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 与 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 又让数据保留更多的原始信息。



BN的优势（结合背景来思考）：

1. **BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度** （训练更快）

   BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。(后一层不会过于依赖前一层，对网络层实现了解耦，就有点**类似**每层单独训练的感觉~ 当然不是真正的解耦)

2. **BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定**（简化了对初始参数的依赖 大参数也可以 利于调参， **不再需要小心的调整学习率和权重初始化**）

   经过BN操作以后，权重的缩放值会被“抹去"  BN就保证了梯度不会依赖于参数的scale, 

   在使用Batch Normalization之后，抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强，此时我们可以设置较大的学习率而不用过于担心模型divergence的风险。

3. **BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题**

4. **BN具有一定的正则化效果**， 可以替代dropout

   类似于Dropout， 使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，不同的batch类似于对网络的学习过程增加了随机噪音，一定程度上起到了正则化的效果



#### early stopping 

L1、L2通常是加入loss function的正则项，它们起到的作用是抑制loss function的复杂度。

dropout则是在神经网络的训练中，随机使得部分神经元失活，从而减小神经网络的复杂度。

当模型在验证集上的表现开始下降的时候，停止训练

没有减少任何复杂度，只是减少了训练迭代的轮次



### 5. 梯度下降方法

梯度下降可以分为：batch，mini-batch，SGD（随机梯度下降）三种，batch方法每次都使用全量训练样本计算本次迭代的梯度方向，mini-batch使用一小部分样本进行迭代，而SGD每次只利用一个样本计算梯度。



### 6. 梯度消失和梯度爆炸

简单地说，根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话（![[公式]](https://www.zhihu.com/equation?tex=w_%7Bij%7Dy_%7Bi%7D%27%3C1.0) ），那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0（![[公式]](https://www.zhihu.com/equation?tex=%5Clim_%7Bn%5Cto%5Cinfty%7D0.99%5En%3D0) ）



**梯度消失和梯度爆炸**

对激活han函数进行**求导部分**大于1（激活函数求导后梯度值），那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生**梯度爆炸**（，梯度变的非常大，然后导致网络权重的大幅更新，并因此使网络变得不稳定），如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了**梯度消失**（会导致靠近输入层的隐藏层权值更新缓慢或者更新停滞）。

**根本原因在于反向传播训练法则，属于先天不足。**



#### BP的影响

BP反向传播的解释 （很棒）温故知新——前向传播算法和反向传播算法（BP算法）及其推导 - G-kdom的文章 - 知乎 https://zhuanlan.zhihu.com/p/71892752

由反向传播算法**链式求导**更新梯度的公式，得知影响梯度更新的几个方面有：**初始权重、激活函数、梯度流动方式、损失值过大**

（1）初始权重带来的影响：神经网络权重初始化不当；

（2）激活函数带来的影响：激活函数选择不当；

（3）梯度流动方式带来的影响：网络结构本身的问题，如RNN；

（4）损失值过大带来的影响：数据集的问题，如标注不准等



##### 初始权重的影响

不当的初始化可能会带来梯度消失或者梯度爆炸， 当网络过深，如果连乘的因子大部分小于1，最后乘积可能趋于0；另一方面，如果连乘的因子大部分大于1，最后乘积可能趋于无穷。这就是所谓的梯度消失与梯度爆炸。

解决办法：

1) 使用Xavier初始化法或者MSRA初始化法，使得在深度网络的每一层，激活值都有很好的分布。

2) 使用预训练模型，初始化已有网络层的权重。

3） bn可以解决初始权重的影响



##### 激活函数的影响

当使用sigmoid或者tanh作为激活函数，当输入很大或者很小的时候，饱和的神经元会带来梯度消失（梯度饱和区）

解决办法：

1. 使用Relu or Leaky Relu
2. bn的使用可以 normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失



##### 网络本身结构问题

RNN的缺点： RNN相当于把许多循环神经网络单元连接成一个序列。对其中某一参数h0梯度的求导表达式将会包含很多很多权重矩阵因子



#### 梯度消失

产生原因（不同解释）： 

1. **不合适的激活函数**  在使用sigmoid激活函数时，中心部位和两侧梯度差别太大，即使初始化很好，也很容易值到达两侧，使得梯度很小，不更新，无法使用梯度更新来使其恢复
2. 初始化参数
3. 本质原因是**反向传播的链式求导的连乘效应**，导致最后对权重的偏导接近于零。



解决办法：

1. Relu激活函数 和 合适的初始化参数方法
2. BN - 无论优化多少层 都和浅层一样， 不容易到达梯度饱和区
3. LSTM



#### 梯度爆炸

产生原因：

1. 和梯度消失类似
2. 过深层的网络 
3. 权值初始化参数过大 



解决办法：

1. **梯度截断** -- 设置一个梯度截断阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。
2. 正则化 l1和l2 (限制网络参数过大 加以惩罚)
3. Relu激活函数 -- 激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度
4. **BN** -- 通过对每一层的输出规范为均值和方差一致的方法，消除了xx*x*带来的放大缩小的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。



#### loss为Nan

训练网络loss出现Nan解决办法 - 我要鼓励娜扎的文章 - 知乎 https://zhuanlan.zhihu.com/p/89588946

模型训练中出现NaN Loss的原因及解决方法 - Celine的文章 - 知乎 https://zhuanlan.zhihu.com/p/114150904



> ## 1 梯度爆炸
>
> - 原因：学习的过程中，梯度变得非常大，使得学习的过程偏离了正常的轨迹。
> - 症状：观察每次迭代的loss值，会发现loss明显增长，最后因为loss值太大以至于不能用浮点去表示，所以变成了Nan。
> - 可采取的措施：1 降低学习速率，2 如果模型中有多个loss层，就需要找到梯度爆炸的层，然后降低该层的loss weight。
>
> ## 2 学习率过高
>
> - 原因：过高的学习率乘上所有的梯度使得所有参数变成无效的值。
> - 症状：观察输出日志，会发现学习率变成nan
> - 可采取的措施：设置合适的学习速率
>
> ## 3 损失函数有误
>
> - 原因：损失函数的计算,如交叉熵损失函数的计算可能出现log(0),所以就会出现loss为Nan的情况
> - 症状: loss逐渐下降,突然出现Nan
> - 可采取的措施: 尝试重现该错误,打印损失层的值进行调试.
>
> ## 4 输入数据有误
>
> - 原因: 你的输入中存在Nan
> - 症状: loss逐渐下降,突然出现Nan
> - 可采取的措施: 逐步去定位错误数据,然后删掉这部分数据. 可以使用一个简单的网络去读取输入,如果有一个数据是错误的,这个网络的loss值也会出现Nan

出现NaN，说明训练**不收敛了 ，发散 **较大可能时 学习速率高 梯度值过大 梯度爆炸

出现原因：

1. 梯度爆炸 loss不断增大 
2. 不当的损失函数  -- 损失层中loss的计算可能导致NaN的出现
3. 不当的输入- 输入数据可能有问题



解决办法：

1. 加入梯度截断
2. 调整学习速率 可能学习速率过大导致不能收敛（找不到极小值点）
3. 调整网络结构
4. 改变每个层的学习率 每个层有自己的学习率





batch size大小会怎么影响收敛速度

**SGD 中 S代表什么，如何理解？**

S即为stochastic，随机梯度是指用来计算梯度的输入数据是随机选取的一部分（batch），而不是所有的数据。使用所有数据一方面计算量巨大，不太现实，另一方面容易陷入局部极小值难以跳出，随机batch的梯度反而增加了跳出局部极限值的可能性，从而获得更好的结果。



