### 深度学习优化器的区别，Adam和SGD 

#### 一个统一的算法优化框架

深度学习优化算法经历了 SGD -> SGDM -> NAG ->AdaGrad -> AdaDelta -> Adam -> Nadam 这样的发展历程。

> 首先定义：待优化参数： ![[公式]](https://www.zhihu.com/equation?tex=w) ，目标函数： ![[公式]](https://www.zhihu.com/equation?tex=f%28w%29) ，初始学习率 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)。
>
> 而后，开始进行迭代优化。在每个epoch ![[公式]](https://www.zhihu.com/equation?tex=t) ：
>
> 1. 计算目标函数关于当前参数的梯度： ![[公式]](https://www.zhihu.com/equation?tex=g_t%3D%5Cnabla+f%28w_t%29)
> 2. 根据历史梯度计算一阶动量和二阶动量：![[公式]](https://www.zhihu.com/equation?tex=m_t+%3D+%5Cphi%28g_1%2C+g_2%2C+%5Ccdots%2C+g_t%29%3B+V_t+%3D+%5Cpsi%28g_1%2C+g_2%2C+%5Ccdots%2C+g_t%29)，
> 3. 计算当前时刻的下降梯度： ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta_t+%3D+%5Calpha+%5Ccdot+m_t+%2F+%5Csqrt%7BV_t%7D)
> 4. 根据下降梯度进行更新： ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bt%2B1%7D+%3D+w_t+-+%5Ceta_t)

步骤3、4对于各个算法都是一致的，主要的差别就体现在1和2上。 即对梯度以及动量的使用上，是否有一阶二阶动量的使用有区别



>  关于一阶动量和二阶动量
>
>  一阶动量是各个时刻梯度方向的指数移动平均值
>
>  二阶动量——该维度上，迄今为止所有梯度值的平方和
>
>  ![[公式]](https://www.zhihu.com/equation?tex=V_t+%3D+%5Csum_%7B%5Ctau%3D1%7D%5E%7Bt%7D+g_%5Ctau%5E2)
>
>  二阶动量用来衡量历史更新频率 ，上面的第3步中： 
>
>  ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta_t+%3D+%5Calpha+%5Ccdot+m_t+%2F+%5Csqrt%7BV_t%7D)
>
>  可以看出，此时实质上的学习率由 ![[公式]](https://www.zhihu.com/equation?tex=+%5Calpha) 变成了 ![[公式]](https://www.zhihu.com/equation?tex=+%5Calpha+%2F+%5Csqrt%7BV_t%7D) 。 一般为了避免分母为0，会在分母上加一个小的平滑项。因此![[公式]](https://www.zhihu.com/equation?tex=%5Csqrt%7BV_t%7D) 是恒大于0的，而且参数更新越频繁，二阶动量越大，学习率就越小。





#### Gradient Descent - 梯度下降

梯度下降是指，在给定待优化的模型参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Ed) 和目标函数 ![[公式]](https://www.zhihu.com/equation?tex=J%28%5Ctheta%29) 后，算法通过沿梯度 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla_%5Ctheta+J%28%5Ctheta%29) 的相反方向更新 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 来最小化 ![[公式]](https://www.zhihu.com/equation?tex=J%28%5Ctheta%29) ， 学习率 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta) 决定了每一时刻的更新步长

计算目标函数关于参数的梯度

![[公式]](https://www.zhihu.com/equation?tex=g_t+%3D+%5Cnabla_%5Ctheta+J%28%5Ctheta%29)



#### SGD (Stochastic Gradient Descent) 

随机梯度下降即 常用的，  最为简单，没有动量的概念 

对参数的更新即为： 

![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_%7Bi%2B1%7D%3D+%5Ctheta_t+-+%5Ceta+g_t)

SGD 的缺点

> 1. 收敛速度慢，可能在鞍点处震荡。 而且可能会在沟壑的两边持续震荡，停留在一个局部最优点
> 2. 如何合理的选择学习率是 SGD 的一大难点。



#### SGD with Momentum （SGD-M）

SGD 在遇到沟壑时容易陷入震荡。为此，可以为其引入一阶动量 Momentum[3]，加速 SGD 在正确方向的下降并抑制震荡。

> 一阶动量是各个时刻梯度方向的指数移动平均值，约等于最近 ![[公式]](https://www.zhihu.com/equation?tex=1%2F%281-%5Cbeta_1%29) 个时刻的梯度向量和的平均值。

参数更新公式：

![[公式]](https://www.zhihu.com/equation?tex=m_t+%3D+%5Cgamma+m_%7Bt-1%7D+%2B+%5Ceta+g_t)

![[公式]](https://www.zhihu.com/equation?tex=m_t+%3D+%5Cbeta_1+%5Ccdot+m_%7Bt-1%7D+%2B+%281-%5Cbeta_1%29%5Ccdot+g_t)

SGD-M 在原步长之上，增加了与上一时刻***步长***相关的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma+m_%7Bt-1%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 通常取 0.9 左右，使得  参数更新方向不仅由当前的梯度决定，也与此前累积的下降方向有关

> 增加了上一时刻的步长的作用：
>
> 1. 在梯度方向变化不大的维度， 加速更新梯度
> 2. 在梯度变化大的维度，减少此次更新的幅度（有上一步的变量做影响）
>
> 即 加速以及抑制震荡



SGD以及SGD-M 的区别：

SGD：

![img](https://pic3.zhimg.com/v2-2476080e4cdfd489ae64ae3ceeafe48b_b.jpg)

SGD-M：

![img](https://pic2.zhimg.com/v2-b9388fd6e465d82687680f9d16edcd2b_b.jpg)



#### SGD with Nesterov Acceleration （NAG）

NAG 是在SGD、SGD-M的基础上的进一步改进  改进点在于步骤1 即梯度的计算

主要思想是：

>  SGD 还有一个问题是困在局部最优的沟壑里面震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。
>
>  
>
>  因此直接不计算当前的梯度，而是使用累计动量走***下一步之后的梯度***

NAG在步骤1，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的梯度下降方向：

![[公式]](https://www.zhihu.com/equation?tex=g_t%3D%5Cnabla+f%28w_t-%5Calpha+%5Ccdot+m_%7Bt-1%7D+%2F+%5Csqrt%7BV_%7Bt-1%7D%7D%29)

然后用下一个点的梯度方向，与历史累积动量相结合，计算步骤2中当前时刻的累积动量。



#### Adagrad 

开始使用二阶动量，自适应学习率 

SGD、SGD-M均是以相同的学习率去更新 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的各个分量 ，而深度学习中大量参数的更新频率是有所区别的：

对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。

>  更新不频繁的参数， 单次步长大一些，能多学习到
>
>  更新频繁的参数，希望单次步长小一些，从而使得参数学习较为稳定，不至于被单样本影响过多

引入了二阶动量：

![[公式]](https://www.zhihu.com/equation?tex=V_t+%3D+%5Csum_%7B%5Ctau%3D1%7D%5E%7Bt%7D+g_%5Ctau%5E2)

学习率等效为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta+%2F+%5Csqrt%7Bv_t+%2B+%5Cepsilon%7D) 。对于此前频繁更新过的参数，其二阶动量的对应分量较大，学习率就较小。

缺点：

> 因为![[公式]](https://www.zhihu.com/equation?tex=%5Csqrt%7BV_t%7D) 是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。



#### AdaDelta / RMSProp

针对adagrad的缺点，即二阶动量的累积使得学习率不断缩小，则不选择累计全部历史二阶动量

即使用最近一段时间窗口内的动量累计

> 改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。
>
> 使用在SGD-M中的指数移动平均值大约就是过去一段时间的平均值：
>
> ![[公式]](https://www.zhihu.com/equation?tex=V_t+%3D+%5Cbeta_2+%2A+V_%7Bt-1%7D+%2B+%281-%5Cbeta_2%29+g_t%5E2)
>
> 避免了二阶动量持续累积、导致训练过程提前结束



#### Adam

Adam 是前述方法的集大成 

SGD-M在**SGD基础**上增加了一阶动量， AdaGrad和AdaDelta在**SGD基础**上增加了二阶动量

则Adam是  在SGD基础上 把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum。

> SGD的一阶动量：
>
> ![[公式]](https://www.zhihu.com/equation?tex=m_t+%3D+%5Cbeta_1+%5Ccdot+m_%7Bt-1%7D+%2B+%281-%5Cbeta_1%29%5Ccdot+g_t)
>
> 加上AdaDelta的二阶动量：
>
> ![[公式]](https://www.zhihu.com/equation?tex=V_t+%3D+%5Cbeta_2+%2A+V_%7Bt-1%7D+%2B+%281-%5Cbeta_2%29+g_t%5E2)

两个超参数 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cbeta_1%2C+%5Cbeta_2)，前者控制一阶动量，后者控制二阶动量。



#### Nadam

同理，按照NAG的思路，对步骤1 增加不是直接计算当前梯度，而是 

![[公式]](https://www.zhihu.com/equation?tex=g_t%3D%5Cnabla+f%28w_t-%5Calpha+%5Ccdot+m_%7Bt-1%7D+%2F+%5Csqrt%7BV_t%7D%29)

之后再结合Adam即可 

即 ： Nesterov + Adam = Nadam

