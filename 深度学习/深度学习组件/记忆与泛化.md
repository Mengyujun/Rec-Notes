###  深度学习中防止过拟合

过拟合就是模型复杂度过高，低偏差高方差

#### 缓解过拟合的方法

> 1. 数据角度
>    1. 获取更多有效的训练数据
>    2. 数据增强 （标准化 随机旋转 随机裁剪 随机噪声等）
>    3. 确保数据的真实有效性 没有出现数据穿越 
> 2. 模型角度 减小模型规模， 规模太大导致模型完美拟合数据，甚至包括噪声 
> 3. 降低特征数量
> 4. 正则化（**正则化是指修改学习算法，使其降低泛化误差而非训练误差**）
>    1. 参数正则化方法（对模型参数加正则化约束）L1， L2范数 其中L1更容易得到**稀疏解** L2范数容易求导优化
>       1. L1 使得更多参数趋于0 从而相当于减小了网络中的权重参数 ，降低了网络复杂度
>       2. L1相当于一个天然的特征选择器 
>    2. 工程技巧-Dropout以及early stopping
> 5. BN
> 6. early stopping
> 7. 集成学习方法（bagging 和 boosting）



#### Dropout

含义操作： **在训练过程中每次按一定的概率（比如50%）随机地“删除”一部分隐藏单元（神经元）**

所谓的“删除” 不是真正的删除神经元，而是将这部分神经元的 激活函数设为 0从而这些神经元不参与计算

注意每一轮次的训练 都会有不同的dropout（不是一次性的）循环中每次都是不同的drop不同的隐藏层，即相当于有多个不同的子网络

![img](https://picb.zhimg.com/v2-79650a2b0124214c0bff9fb01e7460ea_b.jpg)

为什么Dropout会防止过拟合？

1. **取平均**的作用 

   不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。

2. **减少神经元之间复杂的共适应关系** 减少对于某一个子网络 或 特征的过度依赖 

   使得有可能原有神经网络中的两个神经元不会同时出现，避免了对某个特征（某个子网络）的过度依赖，从而降低了过拟合的可能性

   不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。

3. 类似于 环境突变 但仍然能做出及时正确的反应 此时就是鲁棒的 



dropout 在**eval**过程中会 将所有参数都乘以 dropout比率 



####  batch normalizatin

参见很好的**链接**： 

Batch Normalization原理与实战 - 天雨粟的文章 - 知乎 https://zhuanlan.zhihu.com/p/34879333

深度学习中 Batch Normalization为什么效果好？ - 言有三的回答 - 知乎 https://www.zhihu.com/question/38102762/answer/607815171



bn的**背景**是： 

1. 随着训练的进行，一方面有蝴蝶效应，底层网络的参数的小变动可能导致后面的层 的输入有很大的变化，即输入分布有较大的变化
2. 随着网络层数的加深，后面的网络层次 的输入很容易陷入梯度饱和区（此时梯度会变得很小甚至接近于0）（参考sigmoid函数的首尾部分），从而参数更新变慢，网络收敛变慢



BN的**操作**大概是：

在mini-batch的情况下，对每一层的输入【】一个长度为batch-size的list 做标准化 转化为正态分布 

原先输入： 此时的batchsize 为8 

![img](https://pic4.zhimg.com/v2-084e9875d10896369e09af5a60e56250_b.jpg)

做batch normalization 后输入的变化，即输入有了分布

![img](https://pic4.zhimg.com/v2-c37bda8f138402cc7c3dd62c509d36f6_b.jpg)

**用更加简化的方式来对数据进行规范化，使得第 ![[公式]](https://www.zhihu.com/equation?tex=l) 层的输入每个特征的分布均值为0，方差为1。**让每一层网络的输入数据分布都变得稳定（解决之前传递之后输入分布不稳定的情况）

上面的缺陷： 改变了分布 可能导致数据表达能力变差

引入两个参数来改变

BN又引入了两个可学习（learnable）的参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 与 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 。这两个参数的引入是为了恢复数据本身的表达能力，对**规范化**后的数据进行**线性变换**，即 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BZ_j%7D%3D%5Cgamma_j+%5Chat%7BZ%7D_j%2B%5Cbeta_j)

若 当 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%5E2%3D%5Csigma%5E2%2C%5Cbeta%3D%5Cmu) 时，即可恢复原数据分布，从而**一定程度**上保证了输入数据的表达能力



由上可得， bn的**公式**为：

对于神经网络中的第 ![[公式]](https://www.zhihu.com/equation?tex=l) 层，我们有：

![[公式]](https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Cmu%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5EmZ%5E%7B%5Bl%5D%28i%29%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%5E2%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%28Z%5E%7B%5Bl%5D%28i%29%7D-%5Cmu%29%5E2)

![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BZ%7D%5E%7B%5Bl%5D%7D%3D%5Cgamma%5Ccdot%5Cfrac%7BZ%5E%7B%5Bl%5D%7D-%5Cmu%7D%7B%5Csqrt%7B%5Csigma%5E2%2B%5Cepsilon%7D%7D%2B%5Cbeta)

![[公式]](https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D%3Dg%5E%7B%5Bl%5D%7D%28%5Ctilde%7BZ%7D%5E%7B%5Bl%5D%7D%29)

即对每一层的输入 做标准化之后 再加了两个参数 来保证一定的输入数据的表达能力 

总结： 通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 与 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 又让数据保留更多的原始信息。



BN的优势（结合背景来思考）：

1. **BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度** （训练更快）

   BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。(后一层不会过于依赖前一层，对网络层实现了解耦，就有点**类似**每层单独训练的感觉~ 当然不是真正的解耦)

2. **BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定**（简化了对初始参数的依赖 大参数也可以 利于调参， **不再需要小心的调整学习率和权重初始化**）

   经过BN操作以后，权重的缩放值会被“抹去"  BN就保证了梯度不会依赖于参数的scale, 

   在使用Batch Normalization之后，抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强，此时我们可以设置较大的学习率而不用过于担心模型divergence的风险。

3. **BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题**

4. **BN具有一定的正则化效果**， 可以替代dropout

   类似于Dropout， 使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，不同的batch类似于对网络的学习过程增加了随机噪音，一定程度上起到了正则化的效果

BN的缺点呢：？



#### early stopping 

L1、L2通常是加入loss function的正则项，它们起到的作用是抑制loss function的复杂度。

dropout则是在神经网络的训练中，随机使得部分神经元失活，从而减小神经网络的复杂度。

当模型在验证集上的表现开始下降的时候，停止训练

没有减少任何复杂度，只是减少了训练迭代的轮次



### 5. 梯度下降方法

梯度下降可以分为：batch，mini-batch，SGD（随机梯度下降）三种，batch方法每次都使用全量训练样本计算本次迭代的梯度方向，mini-batch使用一小部分样本进行迭代，而SGD每次只利用一个样本计算梯度。



### 6. 梯度消失和梯度爆炸

简单地说，根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话（![[公式]](https://www.zhihu.com/equation?tex=w_%7Bij%7Dy_%7Bi%7D%27%3C1.0) ），那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0（![[公式]](https://www.zhihu.com/equation?tex=%5Clim_%7Bn%5Cto%5Cinfty%7D0.99%5En%3D0) ）



**梯度消失和梯度爆炸**

对激活han函数进行**求导部分**大于1（激活函数求导后梯度值），那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生**梯度爆炸**（，梯度变的非常大，然后导致网络权重的大幅更新，并因此使网络变得不稳定），如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了**梯度消失**（会导致靠近输入层的隐藏层权值更新缓慢或者更新停滞）。

**根本原因在于反向传播训练法则，属于先天不足。**



#### BP的影响

BP反向传播的解释 （很棒）温故知新——前向传播算法和反向传播算法（BP算法）及其推导 - G-kdom的文章 - 知乎 https://zhuanlan.zhihu.com/p/71892752

由反向传播算法**链式求导**更新梯度的公式，得知影响梯度更新的几个方面有：**初始权重、激活函数、梯度流动方式、损失值过大**

（1）初始权重带来的影响：神经网络权重初始化不当；

（2）激活函数带来的影响：激活函数选择不当；

（3）梯度流动方式带来的影响：网络结构本身的问题，如RNN；

（4）损失值过大带来的影响：数据集的问题，如标注不准等



##### 初始权重的影响

不当的初始化可能会带来梯度消失或者梯度爆炸， 当网络过深，如果连乘的因子大部分小于1，最后乘积可能趋于0；另一方面，如果连乘的因子大部分大于1，最后乘积可能趋于无穷。这就是所谓的梯度消失与梯度爆炸。

解决办法：

1) 使用Xavier初始化法或者MSRA初始化法，使得在深度网络的每一层，激活值都有很好的分布。

2) 使用预训练模型，初始化已有网络层的权重。

3） bn可以解决初始权重的影响



##### 激活函数的影响

当使用sigmoid或者tanh作为激活函数，当输入很大或者很小的时候，饱和的神经元会带来梯度消失（梯度饱和区）

解决办法：

1. 使用Relu or Leaky Relu
2. bn的使用可以 normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失



##### 网络本身结构问题

RNN的缺点： RNN相当于把许多循环神经网络单元连接成一个序列。对其中某一参数h0梯度的求导表达式将会包含很多很多权重矩阵因子



#### 梯度消失

产生原因（不同解释）： 

1. **不合适的激活函数**  在使用sigmoid激活函数时，中心部位和两侧梯度差别太大，即使初始化很好，也很容易值到达两侧，使得梯度很小，不更新，无法使用梯度更新来使其恢复
2. 初始化参数
3. 本质原因是**反向传播的链式求导的连乘效应**，导致最后对权重的偏导接近于零。



解决办法：

1. Relu激活函数 和 合适的初始化参数方法
2. BN - 无论优化多少层 都和浅层一样， 不容易到达梯度饱和区
3. LSTM



#### 梯度爆炸

产生原因：

1. 和梯度消失类似
2. 过深层的网络 
3. 权值初始化参数过大 



解决办法：

1. **梯度截断** -- 设置一个梯度截断阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。
2. 正则化 l1和l2 (限制网络参数过大 加以惩罚)
3. Relu激活函数 -- 激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度
4. **BN** -- 通过对每一层的输出规范为均值和方差一致的方法，消除了xx*x*带来的放大缩小的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。



#### loss为Nan

训练网络loss出现Nan解决办法 - 我要鼓励娜扎的文章 - 知乎 https://zhuanlan.zhihu.com/p/89588946

模型训练中出现NaN Loss的原因及解决方法 - Celine的文章 - 知乎 https://zhuanlan.zhihu.com/p/114150904



> ## 1 梯度爆炸
>
> - 原因：学习的过程中，梯度变得非常大，使得学习的过程偏离了正常的轨迹。
> - 症状：观察每次迭代的loss值，会发现loss明显增长，最后因为loss值太大以至于不能用浮点去表示，所以变成了Nan。
> - 可采取的措施：1 降低学习速率，2 如果模型中有多个loss层，就需要找到梯度爆炸的层，然后降低该层的loss weight。
>
> ## 2 学习率过高
>
> - 原因：过高的学习率乘上所有的梯度使得所有参数变成无效的值。
> - 症状：观察输出日志，会发现学习率变成nan
> - 可采取的措施：设置合适的学习速率
>
> ## 3 损失函数有误
>
> - 原因：损失函数的计算,如交叉熵损失函数的计算可能出现log(0),所以就会出现loss为Nan的情况
> - 症状: loss逐渐下降,突然出现Nan
> - 可采取的措施: 尝试重现该错误,打印损失层的值进行调试.
>
> ## 4 输入数据有误
>
> - 原因: 你的输入中存在Nan
> - 症状: loss逐渐下降,突然出现Nan
> - 可采取的措施: 逐步去定位错误数据,然后删掉这部分数据. 可以使用一个简单的网络去读取输入,如果有一个数据是错误的,这个网络的loss值也会出现Nan

出现NaN，说明训练**不收敛了 ，发散 **较大可能时 学习速率高 梯度值过大 梯度爆炸

出现原因：

1. 梯度爆炸 loss不断增大 
2. 不当的损失函数  -- 损失层中loss的计算可能导致NaN的出现
3. 不当的输入- 输入数据可能有问题



解决办法：

1. 加入梯度截断
2. 调整学习速率 可能学习速率过大导致不能收敛（找不到极小值点）
3. 调整网络结构
4. 改变每个层的学习率 每个层有自己的学习率





batch size大小会怎么影响收敛速度

**SGD 中 S代表什么，如何理解？**

S即为stochastic，随机梯度是指用来计算梯度的输入数据是随机选取的一部分（batch），而不是所有的数据。使用所有数据一方面计算量巨大，不太现实，另一方面容易陷入局部极小值难以跳出，随机batch的梯度反而增加了跳出局部极限值的可能性，从而获得更好的结果。

【学界】为什么说随机梯度是一个很好的方法？ - 留德华叫兽的文章 - 知乎 https://zhuanlan.zhihu.com/p/40096385

