### 激活函数的区别

**梯度饱和区，此时梯度会变得很小甚至接近于0**

参见很好的链接：

https://www.jianshu.com/p/22d9720dbf1a

把线性神经网络，转成非线性的，主要优点：

**提高模型鲁棒性，非线性表达能力，缓解梯度消失问题，将特征图映射到新的特征空间从何更有利于训练，加速模型收敛等问题都有很好的帮助**



激活函数的发展经历了**Sigmoid -> Tanh -> ReLU -> Leaky ReLU -> Maxout**这样的过程，还有一个特殊的激活函数**Softmax**，因为它只会被用在网络中的最后一层，用来进行最后的分类和归一化。

![img](https://pic2.zhimg.com/v2-17708ef17113fc120b045db3de3dbaac_b.jpg)







不同激活函数的对比： 

![img](https://pic4.zhimg.com/v2-665f3304d409b17471dd0b7258818e0a_b.jpg)

------

#### Sigmoid

**存在问题：**

- ***Sigmoid函数饱和使梯度消失***。当神经元的激活在接近0或1处时会饱和，在这些区域梯度几乎为0，这就会导致梯度消失，几乎就有没有信号通过神经传回上一层。

- ***Sigmoid函数的输出不是零中心的***。因为如果输入神经元的数据总是正数，那么关于![[公式]](https://www.zhihu.com/equation?tex=w)的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这将会导致梯度下降权重更新时出现z字型的下降。



#### Tanh函数

**存在问题：**

Tanh解决了Sigmoid的输出是不是零中心的问题，但仍然存在饱和问题。

**为了防止饱和，现在主流的做法会在激活函数前多做一步\*batch normalization\*，尽可能保证每一层网络的输入具有均值较小的、零中心的分布。**



#### ReLu 线性整流函数

非饱和性激活函数

比sigmoid类函数主要变化是：

1）单侧抑制；

2）相对宽阔的兴奋边界；

3）稀疏激活性。





#### **Softmax**

**数学公式：**

Softmax用于多分类神经网络输出，目的是让大的更大。函数公式是

![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%28z%29_%7Bj%7D%3D%5Cfrac%7Be%5E%7Bz_%7Bj%7D%7D%7D%7B%5Csum_%7Bk%3D1%7D%5E%7BK%7D%7Be%5E%7Bz_%7Bk%7D%7D%7D%7D%5C%5C)

示意图如下。

![img](https://pic1.zhimg.com/v2-68a7dfdf613d8cd43f0569184b206c5c_b.jpg)Softmax示意图

Softmax是Sigmoid的扩展，当类别数k＝2时，Softmax回归退化为Logistic回归。

就是如果某一个 zj 大过其他 z, 那这个映射的分量就逼近于 1,其他就逼近于 0，主要应用就是多分类。

为什么要取指数，第一个原因是要模拟 max 的行为，所以要让大的更大。第二个原因是需要一个可导的函数。

​	第一个好处就是答主说的**好求导**，第二个就是它使得**好结果和坏结果之间的差异更加显著，更有利于学习**

softmax函数求导的便利： 

Softmax 函数的特点和作用是什么？ - 忆臻的回答 - 知乎 https://www.zhihu.com/question/23765351/answer/240869755

求导形式以及结果十分简单 