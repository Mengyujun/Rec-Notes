### 1. 决策树相关 参见xmind



### 2. LR的进一步总结

​	Youtube- Logistic Regression当做输出层，而是采用了Weighted Logistic Regression。 为了融合优化目标，因为模型使用期望观看时长（expected watch time per impression）作为优化目标，如果简单使用LR就无法引入正样本的观看时长信息。因此采用weighted LR。



### 3. SVM相关知识 

​	参见SVM.md

### 4. knn算法

#### 		1. knn和kmeans的关系

区别：

1. knn是有监督算法（数据集带label）， kmeans是无监督算法（数据集都是不带label的）  相似点：都是要把距离相近的划分为同一类别 
2. Knn中 k 指的是近邻个数 即找最近的k个点 
3. kmeans 中k是最后簇的个数 即最后聚类的个数  即要把所有点分成k类 



#### 		2. knn算法的流程 以及要点

KNN做的就是选出距离目标点黑叉叉距离最近的k个点，然后看这k个点的情况决定 当前目标点的分类情况 因此是有监督算法 

KNN算法最简单粗暴的就是将预测点与所有点距离进行计算，然后保存并排序，选出前面K个值看看哪些类别比较多，则预测的点属于哪类。

要点：

1. 第一个要注意的点——**标准化！标准化！标准化！** 不同特征之间的量纲可能有很大差异， 此时既可以使用 归一化也可以使用标准化的方法，即**极差法**or **标准差**
2. knn没有模型参数 很简单的算法 有超参数k 
3. 超参数k的影响：
   1. k越小 越容易过拟合 即很容易受噪声干扰 模型复杂度高，稳健性差 决策边界崎岖
   2. k越大 容易欠拟合 决策边界变得平滑



### 5. kmeans算法 

#### 算法原理

特点是： 类别的个数是人为给定的，通过一次次重复这样的**选择质心-计算距离后分类-再次选择新质心**的流程，直到我们分组之后所有的数据都不会再变化了，也就得到了最终的聚合结果。



#### 算法过程

> 1. 随机选择k个质心（k也是最终想要的结果分类个数）
> 2. 计算样本到质心的距离，每个样本 选择距离最近的那个质心 归为一类，分为k类
> 3. 计算出每一类新的质心（原来的质心+刚分类完的结果一起计算 - 不一定是）
> 4. 再次重复每个样本计算对质心的距离 ，并重新分类
> 5. 判断新旧聚类是否相同，相同则结束，否则重复2-4步骤 



#### 几大核心问题

1. **初始中心点怎么确定**？

     是初始点的选择 不是k的选择 k的选择使用

   1. kmeans++ 方法： 先随机选择一个中心点C1，之后计算所有点到该点的距离，距离最大的设为下一个中心C2，直至选择完所有的k个中心
   2. 最好的方法： 多选择几个不同的初始点 多跑几次该算法，选择最优

2. kmeans的目标函数

   ![[公式]](https://www.zhihu.com/equation?tex=SSE%3D%5Csum_%7Bi%3D1%7D%5E%7BK%7D+%5Csum_%7Bx+%5Cin+C_%7Bi%7D%7D%5Cleft%28C_%7Bi%7D-x%5Cright%29%5E%7B2%7D)

   其中 C_i即为质心点（聚类中心），x为簇内点

3. **点之间的距离定义**

   欧氏距离

   ![[公式]](https://www.zhihu.com/equation?tex=dist_%7Bed%7D%28X%2CY%29%3D%7C%7CX-Y%7C%7C%7B2%7D%3D%5Csqrt%5B2%5D%7B%28x_%7B1%7D-y_%7B1%7D%29%5E%7B2%7D%2B...%2B%28x_%7Bn%7D-y_%7Bn%7D%29%5E%7B2%7D%7D)

4. **所有点的均值**（新的中心点）怎么算

   就是简单的向量平均

   ![[公式]](https://www.zhihu.com/equation?tex=++++C%3D%28%5Cfrac%7Bx_%7B11%7D%2B...%2Bx_%7B1n%7D%7D%7Bm%7D%2C...%2C%5Cfrac%7Bx_%7Bm1%7D%2B...%2Bx_%7Bmn%7D%7D%7Bm%7D%29)

5. K的选择

   手肘法： 绘制出SSE和K的关系图，选择出变化由快读到平缓的 变化点

   ![img](https://pic3.zhimg.com/v2-eea9be92857d00ac680dac2827d0c483_b.jpg)



#### 优缺点

优点： 简单， 容易实现， 聚类效果好

缺点： k值 初始点的选择问题， 局部最优问题， 受离群值影响大 



### 6. N折交叉验证（用来调参）

![img](https://pic2.zhimg.com/v2-d1b0b782dac2ea4ec3ed07fdc0d665e5_b.jpg)



上图展示的是**5折交叉验证，也就是将已知样本集等分为5份，其中4份作为训练集，1份为验证集，做出5个模型**。

具体来说：

1. 把样本集分成5个小的子集，编号为set1、set2、set3、set4、set5；
2. 先用set1、set2、set3、set4建模，得到model1，并在set5上计算误差error1；
3. 在用set1、set2、set3、set5建模，得到model2，并在set4上计算误差error2；
4. 重复以上步骤，建立5个模型，将5个误差值相加后除以5得到平均误差。

​	





### 7. 正则化 之L1和L2 

![[公式]](https://www.zhihu.com/equation?tex=L_%7B1%7D) 范数： ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7B1%7D+%3D+%5Csum_%7Bi+%3D+1%7D%5E%7Bd%7D%5Clvert+x_i%5Crvert)*（每个元素绝对值之和）*

![[公式]](https://www.zhihu.com/equation?tex=L_%7B2%7D) 范数： ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7B2%7D+%3D+%5CBigl%28%5Csum_%7Bi+%3D+1%7D%5E%7Bd%7D+x_i%5E2%5CBigr%29%5E%7B1%2F2%7D)*（欧氏距离）*  注意欧几里得距离没有前面的系数（也没有系数的道理，没有1/2 或者1/n ）

![[公式]](https://www.zhihu.com/equation?tex=L_%7Bp%7D) 范数： ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7Bp%7D+%3D+%5CBigl%28%5Csum_%7Bi+%3D+1%7D%5E%7Bd%7D+x_i%5Ep%5CBigr%29%5E%7B1%2Fp%7D)

**L1 正则 Lasso regularizer**

![[公式]](https://www.zhihu.com/equation?tex=J%28w%2Cb%29%3D%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi%3D1%7D%5E%7Bm%7DL%28%5Chat%7By%7D%2Cy%29%2B%5Cfrac%7B%5Clambda+%7D%7Bm%7D%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7B1%7D)

- **全部权重** ![[公式]](https://www.zhihu.com/equation?tex=w) **的绝对值的和，再乘以λ/n**
- 凸函数，不是处处可微分
- 得到的是**稀疏解**（最优解常出现在顶点上，且顶点上的 w 只有很少的元素是非零的）
-  L1 最重要的一个特点，**输出稀疏**，会把不重要的特征直接置零，而 L2 则不会
- *因为 L1 天然的输出稀疏性，把不重要的特征都置为 0，所以它也是**一个天然的特征选择器**。*

关于L1 得到稀疏解的解释 可以通过求导得到 解释：

参见： l1正则与l2正则的特点是什么，各有什么优势？ - Andy Yang的回答 - 知乎 https://www.zhihu.com/question/26485586/answer/616029832



**L2 正则 Ridge Regularizer / Weight Decay** 权重衰减

![[公式]](https://www.zhihu.com/equation?tex=J%28w%2Cb%29%3D%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi%3D1%7D%5E%7Bm%7DL%28%5Chat%7By%7D%2Cy%29%2B%5Cfrac%7B%5Clambda+%7D%7B2m%7D%5Cleft+%5C%7C+w+%5Cright+%5C%7C%5E%7B2%7D_%7B2%7D)

- **全部权重**![[公式]](https://www.zhihu.com/equation?tex=w)**的平方和，再乘以λ/2n**
- 凸函数，处处可微分
- 易于优化





### 8. 归一化和标准化的区别

归一化、标准化、区间缩放是对数据进行无量纲化常用的方法



#### 归一化

归一化就是 将数据映射到指定的范围, 用于除去不同维度数据的量纲以及量纲维度

归一化就是这个数减去这一特征下所有数的最小值然后除以极差

常见的方式：

1. **min-max归一化**

   ![img](https://pic1.zhimg.com/v2-4a5b11cf152199a77f3be35f63a27556_b.jpg)

#### 标准化**（Z-score）**

标准化就是这个数减去这一特征下所有数的均值然后除以标准差

让数据形成一个正态分布，即均值为0，且标准差为1的正态分布。

![[公式]](https://www.zhihu.com/equation?tex=x_%7Bnormalization%7D%3D%5Cfrac%7Bx-%5Cbar%7B%5Cmu%7D%7D%7B%5Csigma%7D)

正态分布的3个百分数： **68%，95%，99.7%**  （分别对应和均值相距1，2，3个标准差时， 之间中间大部分数据的概率）

<img src="https://pic4.zhimg.com/v2-c08c84662417b94822f4e220c5f305aa_b.jpg" alt="img" style="zoom: 33%;" />

以上两种方法都是**线性变换**，对输入向量X按比例压缩再进行平移，操作之后原始有量纲的变量变成无量纲的变量。不过它们**不会改变分布本身的形状**



可以改变分布的变换：

1. **正态分布Box-Cox变换**

box-cox变换可以将一个非正态分布转换为正态分布，使得分布具有对称性，变换公式如下：

![img](https://pic2.zhimg.com/80/v2-87b882d7a9c92667be524dc85d77a1bd_720w.jpg)

在这里lamda是一个基于数据求取的待定变换参数，Box-Cox的效果如下。

![img](https://pic1.zhimg.com/80/v2-b4e766817fbb31fc7117a6de32814ce0_720w.jpg)

### 9. 评价指标以及计算 

之前在新浪用到的deepfm中的评价指标：

mse, rmse， mae， accuracy， precise， recall， auc， f1_score 



#### MSE(均方误差 mean-squared-error) 

**SSE（和方差）** 即预测数据和原始数据对应点误差的平方和

在SSE的基础上为了消除 样本数量的影响， 对SSE取样本数量的平均机得到了 MSE （mean）

**MSE  = SSE / N** 

公式为： 

![img](https://pic4.zhimg.com/v2-7955303f4808770bb285399527cef1e9_b.png)

![[公式]](https://www.zhihu.com/equation?tex=J+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%28yi-h_%7B%5Ctheta%7D%28x_i%29%29%5E2+%5C%5C)



#### **RMSE(均方根误差 root-mean-squared-error)**

主要是为了解决**量纲**的问题， MSE和RMSE的区别仅在于是否对 **量纲敏感**

即为MSE的平方根

![[公式]](https://www.zhihu.com/equation?tex=RMSE%3D%5Csqrt%7BMSE%7D%3D%5Csqrt%7B%5Cfrac%7BSSE%7D%7Bn%7D%7D%3D%5Csqrt%7B%5Cfrac%7B1%7D%7Bn%7D%5Csum%7Bi%3D1%7D%5Emwi%28yi-%5Chat%7Byi%7D%29%5E2%7D+%5C%5C)



#### **MAE（平均绝对误差 mean-absolute-error）**

通过英文的名字来记忆 mean-平均  r-root(求根植)  squared（求平方）

![img](https://pic1.zhimg.com/v2-bec1c24d810b2199bd99e8495676bd32_b.png)



#### **经典的评价指标** 

![](https://pic3.zhimg.com/80/v2-a253b01cf7f141b9ad11eefdf3cf58d3_720w.jpg?source=1940ef5c)

recall= tp/tp+fn 即所求的是 预测为真的占 所有真正为真的 比例

precision = tp/ tp+fp  即所求的是 预测为真 占 所有预测真的 比例 

accuracy = tp+tn / total  

真阳率（等于recall） tp-rate = tp / tp+fn   即 所有真实类别为1的样本中，预测类别为1的比例

伪阳率  fp-rate = fp / fp+tn   即所有真实类别为0的样本中，预测类别为1的比例 



### 10. 交叉熵

首先，交叉熵的**公式**明确一下：

这是单个样本的交叉熵公式：

![[公式]](https://www.zhihu.com/equation?tex=L%3D-%5Bylog%5C+%5Chat+y%2B%281-y%29log%5C+%281-%5Chat+y%29%5D)

计算 N 个样本的总的损失函数：

![[公式]](https://www.zhihu.com/equation?tex=L%3D-%5Csum_%7Bi%3D1%7D%5ENy%5E%7B%28i%29%7Dlog%5C+%5Chat+y%5E%7B%28i%29%7D%2B%281-y%5E%7B%28i%29%7D%29log%5C+%281-%5Chat+y%5E%7B%28i%29%7D%29)

为什么总是 y和1-y的形式呢？ 注意，在**二分类问题模型**（其实也是最常见 最多应用的， 即真实的y往往不是0就是1），**逻辑回归，神经网络**，即是最常见的二分类模型， 往往要求的交叉熵公式也是上述



公式推导参见xmind中逻辑回归部分 



二分类是最常见，最常考察的

**多分类**的情况实际上就是对二分类的扩展：

![[公式]](https://www.zhihu.com/equation?tex=L+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%7D+L_i+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%7D+-%5Csum_%7Bc%3D1%7D%5EMy_%7Bic%7D%5Clog%28p_%7Bic%7D%29+%5C%5C)

其中：
\- ![[公式]](https://www.zhihu.com/equation?tex=M) ——类别的数量；
\- ![[公式]](https://www.zhihu.com/equation?tex=y_%7Bic%7D) ——指示变量（0或1）,**如果该类别和样本i的类别相同就是1**，否则是0； 注意这点 因此实际**每次都只有一项**，在二分类中也是一样的，每次只有log(y') 或者log(1-y') 一项，而不是想象中的相加的情况 
\- ![[公式]](https://www.zhihu.com/equation?tex=p_%7Bic%7D) ——对于观测样本i属于类别 ![[公式]](https://www.zhihu.com/equation?tex=c) 的预测概率。



### **11. Pearson相关性系数**（皮尔逊相关系数）

含义： 

1. 其实就是之前计算两个向量之间的相关性 衡量向量相似度的一种方法。输出范围为**-1到+1**, 0代表无相关性，负值为负相关，正值为正相关。
2. 相关系数是研究变量之间**线性相关**程度的量 对相关系数，有了这样的描述：表征两个数列的变化关系（同向或反向的程度），即同时变大则相关系数为正，同时变小则为负。

计算公式： 

![img](https://pic2.zhimg.com/v2-25a444957767f63c41b9ed24854f9908_b.jpg)

公式中 COV为两个变量的协方差，分母为两个变量标准差的乘积。 ![[公式]](https://www.zhihu.com/equation?tex=%CE%BC_X+++) 是X的平均值， ![[公式]](https://www.zhihu.com/equation?tex=%CE%BC_%7BY%7D) 是Y的平均值，E为期望。





其中协方差计算 :

 ![[公式]](https://www.zhihu.com/equation?tex=cov%3DE%5BXY%5D-E%5BX%5DE%5BY%5D)

[计算实例](https://blog.csdn.net/qq_39901989/article/details/102606240?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param)

当协方差值>0时，表明两个数组正相关   值=0时，表明两个数不相关 值<0时，表明两个数组负相关。

对协方差消除量纲的影响（除以 （X和Y的标准差的乘积）），即可得到皮尔逊相关系数 

​	相关系数也可以看成协方差：一种剔除了两个变量量纲影响、标准化后的特殊协方差



### 12. 生成式模型和判别式模型

对于判别式模型来说求得P(Y|X)，对未见示例X，根据P(Y|X)可以求得标记Y，即可以**直接判别**出来

生成式模型求得**P(Y,X)**，对于未见示例X，你要求出X与不同标记之间的**联合概率分布**，然后大的获胜



常见的模型大部分都是判别式模型；

生成式模型有： **朴素贝叶斯模型、隐马尔可夫模型HMM**



举例：

判别式模型举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。

生成式模型举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。

细细品味上面的例子，判别式模型是根据一只羊的特征可以直接给出这只羊的概率（比如logistic regression，这概率大于0.5时则为正例，否则为反例），而生成式模型是要都试一试，最大的概率的那个就是最后结果~



### 13. 朴素贝叶斯模型

**贝叶斯公式：**

**![img](https://pic4.zhimg.com/v2-15b16ce6d37b616a5443c0f7e42e03ec_b.png)**

转化为： 

![img](https://pic4.zhimg.com/v2-a2a73f43adcbb0bf4b9bae19b9495f81_b.png)

**朴素贝叶斯算法是假设各个特征之间相互独立**

则在 p（特征|类别） 中就可以分解开来计算  即 p（特征1,2,3|类别） = p（特征1|类别）* p（特征2|类别）* p（特征3|类别）

具体例子参见： 带你理解朴素贝叶斯分类算法 - 忆臻的文章 - 知乎 https://zhuanlan.zhihu.com/p/26262151



**优点：**

1. 逻辑简单，易于实现
2. 时空开销小

**缺点：**

1. 相互独立的假设过于严格，相关性较大时分类效果不好
2. 对部分概率要用到平滑 避免概率为0 的情况 



### 14. 目标函数，代价函数，损失函数区别

**结论**： **损失函数和代价函数**是同一个东西，目标函数是一个与他们相关但更广的概念，对于目标函数来说在有约束条件下的最小化就是损失函数（loss function）

**对于object在有约束条件下的最小化就是loss**



机器学习中的目标函数、损失函数、代价函数有什么区别？ - 哥德巴赫的猜想的回答 - 知乎 https://www.zhihu.com/question/52398145/answer/209358209

具体：

**损失函数和代价函数**为了表示我们拟合的好坏，我们就用一个函数来**度量拟合的程度**，比如：

![[公式]](https://www.zhihu.com/equation?tex=L%28Y%2Cf%28X%29%29+%3D+%28Y-f%28X%29%29%5E2) 

考虑更多的因素，如正则化，最终的优化函数是：![[公式]](https://www.zhihu.com/equation?tex=min%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DL%28y_%7Bi%7D%2Cf%28x_%7Bi%7D%29%29%2B%5Clambda+J%28f%29) ，即最优化经验风险和结构风险，而这个函数就被称为**目标函数**。



另外一种说法： （代价函数多了多个样本 也可以求平均？--即经验风险最小化）

![image-20200911214708815](https://i.loli.net/2020/09/11/iSw8x9o6NdGMBR5.png)





### 15. 常见的损失函数

1.平方损失函数标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L+%28+Y+%7C+f+%28+X+%29+%29+%3D+%5Csum+_+%7B+N+%7D+%28+Y+-+f+%28+X+%29+%29+%5E+%7B+2+%7D++%5C%5C)

特点：经常应用于回归问题



2.Hinge损失函数标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L%28y%2C+f%28x%29%29+%3D+max%280%2C+1-yf%28x%29%29+++%5C%5C)

特点：

(1)hinge损失函数表示如果被分类正确，损失为0，否则损失就为 ![[公式]](https://www.zhihu.com/equation?tex=1-yf%28x%29) 。**SVM**就是使用这个损失函数。



3. 交叉熵

计算 N 个样本的总的损失函数：

![[公式]](https://www.zhihu.com/equation?tex=L%3D-%5Csum_%7Bi%3D1%7D%5ENy%5E%7B%28i%29%7Dlog%5C+%5Chat+y%5E%7B%28i%29%7D%2B%281-y%5E%7B%28i%29%7D%29log%5C+%281-%5Chat+y%5E%7B%28i%29%7D%29)