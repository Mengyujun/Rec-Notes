# 推荐系统之召回

参考- 深入理解推荐系统：召回 - 鱼遇雨欲语与余的文章 - 知乎 https://zhuanlan.zhihu.com/p/115690499





## 简单介绍

召回： 推荐系统的第一阶段，负责从海量物品库中，快速找回一小部分用户潜在感兴趣的物品，然后交给排序环节

特点：数据量大，速度要求快，因此使用的策略，模型，特征都不能太复杂



上文书提到过，目前工业界推荐系统在召回阶段，大多数采用了**多路召回**策略，比如典型的召回路有：基于用户**兴趣标签**的召回；基于**协同过滤**的召回；基于**热点**的召回；基于**地域**的召回；基于**Topic**的召回；基于**命名实体**的召回等等，除此外还有很多其它类型的召回路。



常见的4种召回方式：

- **基于内容的召回**
- **协同过滤**
- **基于FM模型召回**
- **基于深度神经网络的方法**



## 1.基于内容的召回（CB召回）

### 含义

标签召回，利用**item的具体标签**，以及用户之前的一些历史行为信息（显示或者隐式）获取用户兴趣

注意：是针对该**用户**的，因为该模型**未使用其他用户**的任何信息。



### 优点

1. 针对本用户而言，不需要其他用户数据， 只需要用户本身数据，也即用户本身特征，易扩展到大量用户
2. 可以捕获用户的特定兴趣



### 缺点

1. 由于item的特征表示在某种程度上是手工设计的，因此该技术需要大量领域知识。因此，模型很依赖手工设计特征的好坏
2. 只有记忆能力，泛化能力差，只能学习到已有兴趣，无法扩展用户兴趣



### 基于内容召回的优化点

#### 倒排优化

不太确定具体含义，可以考虑的有倒排指标的选择问题：

1. 选择以ctr倒排--可能出现标题党的问题
2. 选择以完播率倒排--可能出现短视频优先的问题

#### 触发key优化

#### 多维度内容属性



## 2. 协同过滤（CF）

协同过滤召回方式（Collaborative filtering，CF），CF同时使用**user和item之间的相似性**来进行推荐。 这样可以提高模型的推荐**拓展性**。也就是说，协同过滤模型可以根据相似用户B的兴趣向用户A推荐商品。此外，可以自动学习Embedding，而无需依赖手工设计的特征。

3种类型：

- **基于用户(user-based)的协同过滤** 计算用户相似度，对相似用户喜欢的物品，给出当前用户的预测打分，去top-k
- **基于项目(item-based)的协同过滤** 计算item之间的相似度 ，对相似item预测打分 取top-k
- **基于模型（model based）的协同过滤** 最主流的协同过滤类型了，所含算法是非常之多的，如**矩阵分解**、关联算法、聚类算法、**深度学习、图模型**等等。





### 协同过滤（user-cf, item-cf）的缺陷

**user-cf的缺点**：

- 用户数量很大，且会迅速增长，计算用户度相似矩阵时空开销很大
- 用户历史数据非常稀疏，找到相似用户的准确度很低（还是稀疏性）



**user-cf** 有很强的社交特性，适用于发现热点，热点趋势

**item-cf** 适合兴趣变化比较稳定的应用



CF算法的主要缺陷：

- 对大规模稀疏数据，即特征向量稀疏的场景（互联网很常见），**处理稀疏向量能力差**，泛化能力差

- 不利于引入其他信息，user-item-context各种信息的加入 **综合不同特征能力差**
- **相似矩阵**时空**开销**很大



### 基于模型(model based)的协同过滤之矩阵分解（MF）

#### MF的大致流程

矩阵分解是一个简单的Embedding模型。 给定反馈矩阵 ![[公式]](https://www.zhihu.com/equation?tex=A%5Cin+R%5E%7Bm+%5Ctimes+n%7D) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=m) 是用户（或query）数量， ![[公式]](https://www.zhihu.com/equation?tex=n) 是item数量，该模型将学习：

- user Embedding矩阵 ![[公式]](https://www.zhihu.com/equation?tex=U%5Cin%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+d%7D) ，其中第i行是 ![[公式]](https://www.zhihu.com/equation?tex=user_i) 的Embedding。
- item Embedding矩阵 ![[公式]](https://www.zhihu.com/equation?tex=V%5Cin%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+d%7D) ，其中第j行是 ![[公式]](https://www.zhihu.com/equation?tex=item_j) 的Embedding。

![img](https://pic2.zhimg.com/80/v2-b5a4a1594f2e715728c04586ba03f297_720w.jpg)

Embedding通过学习，使得 ![[公式]](https://www.zhihu.com/equation?tex=UV%5ET) 的乘积是反馈矩阵 ![[公式]](https://www.zhihu.com/equation?tex=A) 的良好近似。 ![[公式]](https://www.zhihu.com/equation?tex=UV%5ET) 的 ![[公式]](https://www.zhihu.com/equation?tex=%EF%BC%88i%2Cj%EF%BC%89) 项就是 ![[公式]](https://www.zhihu.com/equation?tex=user_i) 和 ![[公式]](https://www.zhihu.com/equation?tex=item_j) 对应的两个embedding的点积，使其尽量接近 ![[公式]](https://www.zhihu.com/equation?tex=A_%7Bi%2Cj%7D) 。



m*d  和 n * d 其中d表示隐向量的维度， 其中**d的大小决定了隐向量的表达能力的强弱**

d越大，则表达能力越强，泛化程度降低

d越小，则包含信息减少，但泛化能力增大（信息越明确，则泛化能力越低）



#### MF的求解方法

几种不同的矩阵分解方法：

- 特征值分解- 只能用于方阵（要求m=n）

- 奇异值分解（SVD） 

  有缺点： 1. 要求原共现矩阵稠密，实际场景不符合 2.计算复杂度很高

- 梯度下降（GD）



##### 梯度下降方法的过程

损失函数为：就是平方误差损失

![img](https://pic3.zhimg.com/80/v2-7d5912763275f3252f9871ffc9b2f58c_720w.jpg)

目标函数即加上 **正则化**

**随机梯度下降（SGD）**是使损失函数最小化的通用方法。



隐向量的优势： 相对于one-hot，**隐向量的生成其实是对共现矩阵的全局拟合**， 因此利用到了**全局信息**，因此相对于one-hot,有更强的泛化，one-hot如果没有直接的交互，无法计算相似性，也即不具备泛化能力



#### **MF的优点以及局限性**

优点：

- **泛化能力强** （对全局信息的拟合）--解决了CF的**稀疏性问题**
- **空间复杂度低**（参数数量就降低了很多 （n+m）* k大小）-解决了复杂度问题
- **更好的扩展性和灵活性** 思想类似于embedding， 利于和其他特征组合和拼接 

局限性：

- 不方便加入user，item， context等特征-》LR或FM 更好的融合不同特征的能力		-还是**综合各种特征能力差**



## 3. 基于FM的召回

推荐系统召回四模型之：全能的FM模型 - 张俊林的文章 - 知乎 https://zhuanlan.zhihu.com/p/58160982

![img](https://pic4.zhimg.com/80/v2-f7faa46fc88d36677a6311f64ce98beb_720w.jpg)

极简的模型大致流程：

1. 对某用户用之前的FM模型得到的特征embedding， 构建**用户兴趣向量U**

   查询离线训练好的FM模型对应的特征embedding向量，然后将m个物品子集合的特征embedding向量累加，形成**物品向量I**

2. 每个用户的兴趣向量离线算好，存入在线数据库中比如Redis

   物品的向量逐一离线算好，存入Faiss(Facebook开源的embedding高效匹配库)数据库

3. 用户登录后，依据uid 得到用户对应的兴趣向量embedding， 和Faiss中存储的物料embedding做内积计算，然后取top-k



## 4. 基于深度学习网络的召回

























